### SGD 和 Adam 的区别

**什么是优化算法？**

在深入了解这两种算法之前，我们先明确一下什么是**优化算法**

在训练神经网络时，我们的目标是最小化**损失函数**（Loss Function）。损失函数衡量了模型预测结果与真实值之间的差距。优化算法就是一种方法，它告诉我们**如何调整模型的参数**（即权重和偏置），以使损失函数的值越来越小，从而让模型变得越来越准确

可以把优化算法想象成在下山时，如何选择每一步的方向和步长，才能最快地到达山谷（即损失函数的最小值）

**1. 随机梯度下降（SGD, Stochastic Gradient Descent）**

**基本原理**

**SGD** 是最基础、最原始的优化算法。它的“随机”体现在：在每一次迭代中，它**随机选择一个样本**（或一小批样本，即 **Mini-Batch SGD**），然后计算这一个（或一小批）样本的损失，并根据这个损失来更新模型的参数

这与传统的**批量梯度下降**（Batch Gradient Descent）不同，后者会计算所有训练样本的损失来更新一次参数。

**优点**

- **计算效率高**：由于每次只处理一小批样本，计算量大大减少，尤其是在面对海量数据时
- **跳出局部最优**：SGD 的“随机性”使得它有机会跳出一些浅的局部最优解，找到更好的全局最优解。

**缺点**

- **收敛速度慢且不稳定**：由于每次的梯度只代表了一小部分数据，更新方向可能会非常“抖动”和不稳定，导致损失函数在下降时像锯齿一样波动
- **需要手动设置学习率**：如果学习率（步长）太大，可能无法收敛；如果太小，收敛速度会非常慢。而且，整个训练过程都使用同一个固定的学习率，无法根据参数的重要性进行调整

**2. Adam（Adaptive Moment Estimation）**

**基本原理**

**Adam** 是一种**自适应学习率**的优化算法。它结合了**RMSprop** 和 **Adagrad** 的优点，是目前最常用、效果最好的优化器之一。它的核心思想是：**为每个参数都动态地调整学习率**

Adam 主要通过计算梯度的**一阶矩**（均值）和**二阶矩**（方差）的指数移动平均值来调整学习率

- **一阶矩（mt）**：可以看作是梯度的加权平均，它决定了更新方向
- **二阶矩（vt）**：可以看作是梯度平方的加权平均，它决定了更新的步长大小

Adam 会根据这些动态计算的矩，为不同的参数提供不同的学习率。对于梯度大的参数，它会减小学习率；对于梯度小的参数，它会增加学习率

**优点**

- **收敛速度快**：由于它能够自适应地调整学习率，Adam 在多数情况下比 SGD 收敛得更快
- **无需手动调优学习率**：大多数情况下，使用 Adam 的默认参数就能取得很好的效果，大大简化了调参过程
- **能处理稀疏梯度**：对于有稀疏梯度的任务（如自然语言处理），Adam 表现出色

**缺点**

- **可能会收敛到局部最优**：一些研究表明，Adam 在某些特定场景下可能会收敛到比 SGD 差的局部最优解，这是因为它自适应的学习率可能导致它在训练后期学习率过低，无法跳出局部最优

| 特性     | SGD                                              | Adam                                           |
| -------- | ------------------------------------------------ | ---------------------------------------------- |
| 学习率   | 固定，需要手动设置                               | 自适应，为每个参数动态调整                     |
| 更新方式 | 根据单个样本或 mini-batch 的梯度直接更新         | 结合一阶矩和二阶矩，动态调整更新方向和步长     |
| 收敛性   | 波动较大，可能收敛到全局最优                     | 平稳且快速收敛，但可能陷入较差的局部最优       |
| 适用性   | 在大型模型或特定任务上需要精细调参，效果可能更好 | 绝大多数场景下都能快速获得不错的效果，易于使用 |
| 调参难度 | 高，需要仔细调整学习率                           | 低，默认参数通常表现良好                       |