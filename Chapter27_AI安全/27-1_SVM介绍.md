### 介绍下 SVM

**SVM 的核心思想：寻找最佳分隔超平面**

想象一下，你有一堆红色的球和蓝色的球，你想用一根线把它们分开。你可以画出无数条线，但哪条线是最好的呢？

SVM 的答案是：那条离**最近**的红色球和蓝色的球都最远的线

在二维空间中，这个“线”就是一个**分隔平面**（Separating Plane）。在三维空间中，它是一个平面。在更高维度的空间中，我们称它为**超平面**（Hyperplane）

- **支持向量（Support Vectors）**：这些“最近”的数据点就是**支持向量**。它们是距离超平面最近的那些训练样本点，它们决定了超平面的位置。如果移除或移动这些支持向量，超平面的位置可能会发生变化。但如果移动那些离超平面很远的数据点，超平面则保持不变
- **最大间隔（Maximal Margin）**：超平面到最近支持向量的距离被称为**间隔**（Margin）。SVM 的目标就是找到一个超平面，使得这个间隔最大化。这个最大间隔的超平面就是最优超平面

**线性可分与非线性可分**

**1. 线性可分（Linear SVM）**

当数据可以被一条直线（或一个超平面）完美分开时，我们称其为**线性可分**。在这种情况下，我们可以直接使用上面提到的最大间隔方法来找到最优超平面。

**2. 非线性可分（Kernel SVM）**

大多数情况下，数据点并非线性可分。例如，红色球和蓝色球可能混合在一起，你无法用一条直线将它们完全分开

这就是 SVM 强大之处的体现：**核技巧**（Kernel Trick）

核技巧的核心思想是，将原始的低维数据**映射**到一个更高维度的空间，在这个高维空间中，数据变得**线性可分**了

举个例子：一个二维的圆环，内圈是蓝色，外圈是红色。在二维平面上，你无法用一条直线将它们分开。但是，如果我们将这些点映射到三维空间，它们可能就会在一个平面上呈“碗状”分布，这时我们就可以用一个平面将它们分开了

常用的核函数有：

- **线性核（Linear Kernel）**：当数据本身线性可分时使用
- **多项式核（Polynomial Kernel）**：可以将低维数据映射到高维
- **径向基函数核（RBF Kernel）**：也叫高斯核，这是最常用、最强大的核函数之一，它能够将样本映射到无穷维度的空间，从而处理非常复杂的数据集

**软间隔与惩罚参数 C**

在现实世界中，完美的分隔几乎不存在，数据集中往往会有一些“异常值”或“噪声”点，它们会混入另一个类别。如果我们坚持找到一个完美的超平面，模型可能会**过拟合**

为了解决这个问题，SVM 引入了**软间隔（Soft Margin）**的概念。它允许一些数据点“越界”，即它们可以出现在间隔内部，甚至位于错误的一侧

- **惩罚参数 C（Penalty Parameter C）**：这个参数用来控制**容忍度**
  - **C 值很小**：模型容忍度高，允许更多数据点越界。这会使间隔更大，但可能会导致一些误分类，因此更不容易过拟合
  - **C 值很大**：模型容忍度低，对错误分类的惩罚很高。这会使间隔变小，模型试图精确地分离所有训练数据，因此更容易过拟合

选择合适的 C 值是一个重要的调优步骤