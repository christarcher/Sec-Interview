### 如何找攻击样本

**1. 基于梯度的攻击**

这是最常见、最基础的一类攻击方法，利用模型的梯度信息来生成扰动

- **原理**：利用反向传播，计算损失函数相对于输入图像的梯度。这个梯度表示了如果对图像的像素进行微小改变，会如何影响模型的预测结果。通过沿着这个梯度方向对图像进行调整，可以最大化模型的损失，从而导致模型分类错误
- **代表算法**：
  - **快速梯度符号法（FGSM, Fast Gradient Sign Method）**：一种非常高效的单步攻击。它计算损失函数对输入图像的梯度，然后沿着梯度的正负号方向给图像加上一个微小的扰动
  - **基本迭代方法（BIM, Basic Iterative Method）/FGSM 迭代版**：FGSM 的多次迭代版本。它在每一步迭代中都计算梯度并进行小幅调整，以更精确地将图像推向模型的决策边界之外
- **优点**：计算效率高，生成速度快
- **缺点**：生成的对抗样本可能不够隐蔽，有时会留下人眼可见的痕迹

**2. 基于优化的攻击**

这类方法将寻找对抗样本看作一个优化问题，旨在找到最小的扰动，同时确保模型分类失败

- **原理**：设定一个优化目标，比如最小化扰动的大小（通常用L2或L∞范数来衡量），同时满足模型对新样本的预测结果是错误的
- **代表算法**：
  - **Carlini & Wagner（C&W）攻击**：这是最强大的白盒攻击之一。它通过一个精心设计的损失函数，可以生成非常小且难以被防御方法检测到的对抗样本。这种攻击的成功率非常高，但计算成本也相对较高
- **优点**：生成的对抗样本扰动非常小，视觉效果极佳，难以被发现
- **缺点**：计算复杂，生成速度慢

**3. 基于生成模型的攻击**

这类方法利用生成对抗网络（GAN）来生成对抗样本，而非直接基于梯度或优化

- **原理**：训练一个生成器网络，它可以将随机噪声或正常样本作为输入，直接生成对抗性扰动。这个生成器被训练来欺骗一个分类器（判别器）
- **优点**：一旦生成器训练好，生成对抗样本的速度非常快，几乎可以实时生成
- **缺点**：生成器的训练过程复杂且不稳定，可能需要大量数据和计算资源

**4. 黑盒攻击**

以上方法都是**白盒攻击**，即需要知道模型的内部结构和参数。而**黑盒攻击**则更具挑战性，它不需要知道模型的内部信息，只能通过输入和输出来寻找攻击样本

- **原理**：
  - **基于迁移性（Transferability）**：在白盒模型（通常是公开的、结构相似的模型）上生成对抗样本，然后将这些样本用于攻击目标黑盒模型。研究表明，对抗样本在不同模型之间具有一定的**迁移性**
  - **基于查询（Query-based）**：通过向目标模型发送大量查询，观察其输出，并利用这些信息来估计模型的决策边界，从而生成攻击样本。这种方法通常需要大量的查询次数，但效果也更精确
- **优点**：适用于无法获取模型参数的实际应用场景
- **缺点**：通常比白盒攻击效率低，成功率也不如白盒攻击高