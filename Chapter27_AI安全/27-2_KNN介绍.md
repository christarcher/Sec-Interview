### 介绍下 KNN

**KNN 的核心思想：近朱者赤，近墨者黑**

KNN 算法的原理非常直观，可以比喻成“物以类聚，人以群分”

想象一下，你有一张地图，上面标注了各种餐馆的类型（中餐、西餐、日料）。现在，你想在地图上给一个新的、未标注的餐馆进行分类

KNN 的做法是：

1. **找到距离最近的邻居**：首先，找到离这个新餐馆最近的 K 个已知的餐馆
2. **统计邻居的类别**：然后，统计这 K 个最近邻居中，哪种餐馆类型出现的次数最多
3. **做出预测**：将出现次数最多的那个类别，作为新餐馆的预测类别

在这个例子中，**K** 就是你选择的“邻居”数量。如果 K=3，你就会考察离新餐馆最近的 3 个餐馆的类型；如果 K=5，你就会考察 5 个

**KNN 算法的步骤**

1. **确定 K 值**：选择一个合适的整数 K。这个 K 值是算法中唯一也是最重要的参数
2. **计算距离**：对于每一个待分类的样本，计算它与所有训练集中样本的距离。常见的距离度量有：
   - **欧氏距离（Euclidean Distance）**：两点之间直线距离，最常用
   - **曼哈顿距离（Manhattan Distance）**：两点在坐标轴上移动的距离之和
3. **排序**：将计算出的所有距离进行升序排序
4. **选择 K 个最近邻**：选取距离最小的前 K 个样本
5. **投票或取均值**：
   - **分类任务**：统计这 K 个样本所属类别中出现频率最高的那个类别，作为最终的预测类别
   - **回归任务**：计算这 K 个样本的标签值的平均值，作为最终的预测值

**如何选择 K 值？**

K 值的选择对 KNN 算法的性能影响很大：

- **K 值太小**：模型会变得非常复杂，对噪声数据和异常点非常敏感，容易导致**过拟合**
- **K 值太大**：模型会变得过于简单，忽略了局部数据的特点，可能导致**欠拟合**

通常情况下，K 的值会根据数据的具体情况进行选择，常用的方法是通过交叉验证（Cross-validation）来寻找最优的 K 值